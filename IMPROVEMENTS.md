# SerriformNet Improvements

## Implemented Improvements

### Recurrence Enhancements
- âœ… Precomputed decay masks once per max_seq_len, cached for reuse
- âœ… Fixed tensor dimension handling in the exponential decay computation
- âœ… Added max_seq_len parameter for better context control

### Mixture of Experts Enhancements
- âœ… Added router noise for improved training stability
- âœ… Implemented GShard-style load balancing loss
- âœ… Added expert usage tracking
- âœ… Support for entropy-based loss term to encourage diversity
- âœ… Configurable router noise and load balancing weight

### Generation Optimizations
- âœ… Added support for streaming token generation
- âœ… Implemented in-place cache updates to reduce memory allocations
- âœ… Added HuggingFace-compatible past_key_values support

### Architecture Flexibility
- âœ… Added comprehensive architecture configuration
- âœ… Support for ablation studies (conv-only, recurrence-only, moe-only)
- âœ… Modular component toggling
- âœ… Configurable model parameters (kernel size, dilation, experts count)
- âœ… Combining weights for multi-component fusion

### Training and Monitoring
- âœ… Added gradient norm tracking per layer group
- âœ… Support for auxiliary loss term visualization
- âœ… Expert usage entropy monitoring

## Future Roadmap

### Recurrence Improvements
- ðŸ”² Rewrite recurrence with implicit recurrence (Mamba SSM-style filtering)
- ðŸ”² Implement state-space kernel convolution

### MoE Improvements
- ðŸ”² Implement soft routing + sparse dropout
- ðŸ”² Add dynamic expert pruning
- ðŸ”² Support conditional computation with adaptive routing

### Serriform Block Optimizations
- âœ… Support for ablations (conv-only, recurrence-only, moe-only)
- âœ… Support for selective component fusion (e.g., conv + moe)
- ðŸ”² Monitor and optimize FLOPs per block
- ðŸ”² Deeper ablation studies on component combinations

### Performance Optimizations
- ðŸ”² CUDA kernels for custom fused operations
- ðŸ”² Add support for 8-bit quantization
- ðŸ”² Implement fused optimizers (AdamW, Lion, etc.)

### Training Enhancements
- âœ… Track gradient norms per-layer
- âœ… Add entropy-based loss term for expert diversity
- ðŸ”² Implement fused optimizers for large model training

### Architecture Experiments
- ðŸ”² Add conditional gating between components
- ðŸ”² Implement multi-resolution convolution paths
- ðŸ”² Try Gaussian Error Linear Unit variations

## Tracking Progress from Original TODO List

### ðŸ”§ Recurrence
- âœ… Precompute decay masks once per max_seq_len, cache them
- ðŸ”² Rewrite recurrence with implicit recurrence ala Mamba (SSM-style filtering)
- ðŸ”² Use state-space kernel convolution

### ðŸ”§ MoE
- âœ… Add router noise + load balancing loss (GShard-style aux loss)
- ðŸ”² Optionally use soft routing + sparse dropout
- âœ… Track expert usage, penalize overuse

### ðŸ”§ Serriform Block
- âœ… Support for ablations (conv-only, recurrence-only, moe-only, fusion of just two)
- ðŸ”² Monitor FLOPs and runtime per block

### ðŸ”§ Generation
- âœ… Support past_key_values-style streaming for faster autoregressive use
- âœ… Shift cache update to be in-place to reduce allocations

### ðŸ”§ Training
- âœ… Track gradient norms per-layer (for exploding grads)
- âœ… Plug in entropy-based loss term to encourage diversity in expert usage
- ðŸ”² Consider fused optimizer for large models

### ðŸ”§ Modularity
- âœ… Split serriform block into submodules that can be toggled
- âœ… Added arch_config.json style configuration 